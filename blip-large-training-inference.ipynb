{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index --no-deps /kaggle/input/lavis-pretrained/salesforce-lavis/transformers* \n",
    "!pip install --no-index --no-deps /kaggle/input/lavis-pretrained/salesforce-lavis/hugging*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --force-reinstall charset-normalizer==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Credits Section:**\n",
    "> This notebook was made possible by incorporating serveral parts of other public notebooks.\n",
    "> - [[Pytorch] BLIP Training](https://www.kaggle.com/code/debarshichanda/pytorch-blip-training) By [Debarshi Chanda](https://www.kaggle.com/debarshichanda)\n",
    "> - [Post-Processing: Adding Modifiers](https://www.kaggle.com/code/yawata/post-processing-adding-modifiers) By [kyoukuntaro](https://www.kaggle.com/yawata)\n",
    "\n",
    "## BLIP Large - Training & Inference\n",
    "This notebook serves as a simple baseline for training and inference with [BLIP Large](https://huggingface.co/Salesforce/blip-image-captioning-large) model.\n",
    "\n",
    "**BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**\n",
    "\n",
    "<img src =\"https://s3.amazonaws.com/moonup/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif\">\n",
    "\n",
    "BLIP Models are models designed to **Caption images: Generate a simple text describing what is in the image** which is exactly the reverse operation of text->image that is used in stable diffusion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training or Inference?\n",
    "\n",
    "This notebook supports two modes of opertation: Training & Inference (Since we can't use the internet when submitting).\n",
    "This flag is used to switch between the two different modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "SEED = 2023\n",
    "T_MAX = 500\n",
    "MIN_LR = 1e-6\n",
    "N_ACCUMULATE = 1\n",
    "WEIGHT_DECAY = 1e-6\n",
    "LEARNING_RATE = 1e-5  # small lr because we're fine-tuning\n",
    "VALID_BATCH_SIZE = 8\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "SCHEDULER = 'CosineAnnealingLR'\n",
    "DATASET = 'poloclub/diffusiondb'\n",
    "MODEL_NAME = \"Salesforce/blip-image-captioning-large\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EMBEDDING_LENGTH = 384\n",
    "TRAINED_MODEL_PATH = '/kaggle/input/blip-trained-large/blip_trained_large.pt'\n",
    "OFFLINE_BACKBONE_PATH = \"/kaggle/input/blip-pretrained-model/blip-image-captioning-large\"\n",
    "SENTENCE_TRANSFORMERS_MODEL = '/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode 1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from torch.optim import lr_scheduler\n",
    "from transformers import AutoProcessor, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipForConditionalGeneration\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    \n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "    def __len__(self): return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        print(item['image'], item['prompt'])\n",
    "        processed_item = self.processor(images=item[\"image\"], text=item[\"prompt\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        for k, v in processed_item.items():\n",
    "            print(k, v.shape)\n",
    "        print()\n",
    "        return {k:v.squeeze() for k,v in processed_item.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "- **Train one epoch:** Trains one epoch.\n",
    "- **Validate one epoch:** Runs a validation run for one epoch over the dataset.\n",
    "- **Run Training:** Run full training (multiple epochs) over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        pixel_values = data['pixel_values'].to(device)        \n",
    "        batch_size = input_ids.size(0)\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)                \n",
    "        loss = outputs.loss\n",
    "        loss = loss / N_ACCUMULATE\n",
    "        loss.backward()    \n",
    "        if (step + 1) % N_ACCUMULATE == 0:\n",
    "            optimizer.step()            \n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None: scheduler.step()                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size        \n",
    "        epoch_loss = running_loss / dataset_size        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        pixel_values = data['pixel_values'].to(device)        \n",
    "        batch_size = input_ids.size(0)\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)                \n",
    "        loss = outputs.loss        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size        \n",
    "        epoch_loss = running_loss / dataset_size        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0]['lr'])    \n",
    "    gc.collect()    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, num_epochs):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=train_loader, device=DEVICE, epoch=epoch)\n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=DEVICE, epoch=epoch)\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss            \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), f\"BestLoss.bin\")\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    model.load_state_dict(best_model_wts)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    dataset = load_dataset(DATASET, '2m_first_5k')\n",
    "    dataset = dataset['train']\n",
    "    dataset = dataset.filter(lambda example: example[\"step\"] == 50)\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = ImageCaptioningDataset(dataset['train'], processor)\n",
    "    valid_dataset = ImageCaptioningDataset(dataset['test'], processor)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=VALID_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading & Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    model = BlipForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_MAX, eta_min=MIN_LR)\n",
    "    model = run_training(model, optimizer, scheduler, num_epochs=EPOCHS)\n",
    "    del train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "    torch.save(model.state_dict(), 'blip_trained_large.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode 2: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def load_image_paths(image_dir, image_ids):\n",
    "    image_path_list = []\n",
    "    image_set = set()\n",
    "    for image_id in image_ids:\n",
    "        if image_id in image_set: continue\n",
    "        image_set.add(image_id)\n",
    "        image_path_list.append(f\"{image_dir}/{image_id}.png\")\n",
    "    return image_path_list\n",
    "\n",
    "def make_batches(l, batch_size=16):\n",
    "    for i in range(0, len(l), batch_size):\n",
    "        yield l[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')\n",
    "    sample_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the trained BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    processor = AutoProcessor.from_pretrained(OFFLINE_BACKBONE_PATH)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(OFFLINE_BACKBONE_PATH)\n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH))\n",
    "    model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the full pipeline using a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    image_path_list = load_image_paths(\"/kaggle/input/stable-diffusion-image-to-prompts/images\", [val.split(\"_\")[0] for val in sample_submission.index])\n",
    "    raw_image = Image.open(image_path_list[5]).convert(\"RGB\")\n",
    "    pixel_values = processor(images=[raw_image], return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "    out = model.generate(pixel_values=pixel_values, max_length=20, min_length=5)\n",
    "    prompts = processor.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing our sentence transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    st_model = SentenceTransformer(SENTENCE_TRANSFORMERS_MODEL)\n",
    "    images = os.listdir(comp_path/\"images\")\n",
    "    image_ids = [i.split('.')[0] for i in images]\n",
    "    eIds = list(range(EMBEDDING_LENGTH))\n",
    "    imgId_eId = [\n",
    "        '_'.join(map(str, i)) for i in zip(\n",
    "            np.repeat(image_ids, EMBEDDING_LENGTH),\n",
    "            np.tile(range(EMBEDDING_LENGTH), len(image_ids)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    submissions = []\n",
    "    for batch in make_batches(images, BATCH_SIZE):\n",
    "        images_batch = []\n",
    "        for i, image in enumerate(batch): images_batch.append(Image.open(comp_path/\"images\"/image).convert(\"RGB\"))\n",
    "        pixel_values = processor(images = images_batch, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "        out = model.generate(pixel_values=pixel_values, max_length=20, min_length=5)\n",
    "        prompts = processor.batch_decode(out, skip_special_tokens=True)\n",
    "        prompts = [p for p in prompts]\n",
    "        embeddings = st_model.encode(prompts).flatten()\n",
    "        submissions.extend(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    submission = pd.DataFrame({\"imgId_eId\":imgId_eId, \"val\": submissions})\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    sample_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_prompts37",
   "language": "python",
   "name": "sd_prompts37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
