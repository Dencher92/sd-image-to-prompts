import os
import sys
import time
from PIL import Image
from pathlib import Path
import matplotlib.pyplot as plt
import yaml
from tqdm import tqdm
import cv2

import numpy as np
import pandas as pd
import torch

import open_clip
from sentence_transformers import SentenceTransformer, models
from transformers import AutoProcessor, BlipForConditionalGeneration


# Ansabling based on three notebooks:
# 1) Blip: https://www.kaggle.com/code/bibanh/blip-0-40057-baseline-blip-large-pretrained
# 2) Coca: https://www.kaggle.com/code/leonidkulyk/lb-0-42118-laion-s-coca-vit-openclip
# 3) Clip: https://www.kaggle.com/code/leonidkulyk/lb-0-45836-blip-clip-clip-interrogator

# Added:

# - blip:
#   - large -> base+large
#   - max_length -> 75
#   - beam search
#   - multiple prompts generated by each model
#   - added "good" phrases: fine details / masterpiece
#   - removed "bad" tokens: this is / there is

# - coca:
#   - large -> base+large
#   - batching (speed up)
#   - multiple prompts generated by each model (source code lil fix), - but no boost


def main():
    # read paths:
    # with open('paths.yml', 'r') as f:
    #     paths = yaml.safe_load(f)
    #     weights_dir = paths['weights']
    #     input_dir = paths['input']
    #     output_dir = paths['output']
    #     images_dir   = paths['images']

    weights_dir = '/kaggle/input/'
    input_dir = '/kaggle/input/'
    output_dir = './'
    images_dir = '/kaggle/input/stable-diffusion-image-to-prompts/images'



    class CFG:
        device = "cuda"
        seed = 42
        embedding_length = 384
        sentence_model_path = f"{weights_dir}/sentence-transformers-222/all-MiniLM-L6-v2"

        coca_model_1_name = "coca_ViT-L-14"
        coca_model_1_checkpoint_path = f"{weights_dir}/open-clip-models/mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k.bin"  # 638m params (~2.5gb)
        coca_1_bs = 60

        coca_model_2_name = "coca_ViT-L-14"
        coca_model_2_checkpoint_path = f"{weights_dir}/coco-vit-l/open_clip_pytorch_model.bin"  # 253m params (~1gb)
        coca_2_bs = 45

        blip_model_large_path = f"{weights_dir}/blip-pretrained-model/blip-image-captioning-large" # 470m params (~2gb)
        blip_large_bs = 25

        blip_model_base_path = f"{weights_dir}/blip-base-lfs/blip-image-captioning-base" # 247m params (1020mb)
        blip_base_bs = 45

        clip_model_name = "ViT-H-14" # 633m params (~2.6gb)
        clip_model_path = f"{weights_dir}/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin" # 1b params (including 633vit) (~4gb)
        clip_batch_size = 128

        mediums_pickle_path = f'{weights_dir}/clip-interrogator-models-x/ViT-H-14_laion2b_s32b_b79k_mediums.pkl'
        movements_pickle_path = f'{weights_dir}/clip-interrogator-models-x/ViT-H-14_laion2b_s32b_b79k_movements.pkl'
        flavors_pickle_path = f'{weights_dir}/clip-interrogator-models-x/ViT-H-14_laion2b_s32b_b79k_flavors.pkl'

        num_return_sequences = 3
        num_beams = 6
        min_length = 5
        max_length = 75
        num_beam_groups = 3

    # load image paths:

    images = os.listdir(images_dir)
    # TODO: for debugging, remove: (in the hidden test there will be 16k images, and you should not exceed the 9h limit and 16GB gpu RAM)
    images = images * 20

    imgIds = [i.split('.')[0] for i in images]
    eIds = list(range(CFG.embedding_length))

    imgId_eId = [
        '_'.join(map(str, i)) for i in zip(
            np.repeat(imgIds, CFG.embedding_length),
            np.tile(range(CFG.embedding_length), len(imgIds))
        )
    ]


    print('Number of images:', len(images))

    # Define device:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)

    def make_batches(l, batch_size=16):
        for i in range(0, len(l), batch_size):
            yield l[i:i + batch_size]
    # todo: make a pytorch dataloader with multiprocessing for speed up? (kaggle only has 4 cpus.. so may be not)

    # CLIP:
    clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if device == 'cuda' else 'fp32')
    open_clip.load_checkpoint(clip_model, CFG.clip_model_path)
    clip_model.to(device).eval()
    print('clip params:', sum(p.numel() for p in clip_model.parameters() if p.requires_grad))

    # Get Clip embeddings for keywords:
    cos = torch.nn.CosineSimilarity(dim=1)

    mediums_data = np.load(CFG.mediums_pickle_path, allow_pickle=True)
    movements_data = np.load(CFG.movements_pickle_path, allow_pickle=True)
    flavors_data = np.load(CFG.flavors_pickle_path, allow_pickle=True)

    mediums_labels = mediums_data['labels']
    movements_labels = movements_data['labels']
    flavors_labels = flavors_data['labels']

    mediums_features_array = torch.stack([torch.from_numpy(t) for t in mediums_data['embeds']]).to(device)
    movements_features_array = torch.stack([torch.from_numpy(t) for t in movements_data['embeds']]).to(device)
    flavors_features_array = torch.stack([torch.from_numpy(t) for t in flavors_data['embeds']]).to(device)

    clip_mean = getattr(clip_model.visual, 'image_mean', None)
    clip_std = getattr(clip_model.visual, 'image_std', None)
    clip_transform = open_clip.image_transform(
        clip_model.visual.image_size,
        is_train = False,
        mean = clip_mean,
        std = clip_std,
    )

    # timing:
    start_time = time.time()

    clip_texts = []
    for batch in tqdm(make_batches(images, batch_size=CFG.clip_batch_size)):  # 1.2 it/sec for bs=32
        images_batch = []
        for i, image_name in enumerate(batch):
            img = Image.open(os.path.join(images_dir, image_name)).convert("RGB")
            images_batch.append(clip_transform(img).unsqueeze(0))
        images_batch = torch.cat(images_batch,0).to(device)

        with torch.no_grad(), torch.cuda.amp.autocast():
            image_features = clip_model.encode_image(images_batch)
            image_features /= image_features.norm(dim=-1, keepdim=True)

        # for small batches it's okay to just loop through them, lazy to implement batching:
        for i in range(len(image_features)):
            medium = [mediums_labels[i] for i in cos(image_features[i], mediums_features_array).topk(1).indices][0]
            movement = [movements_labels[i] for i in cos(image_features[i], movements_features_array).topk(1).indices][0]
            flaves = ", ".join([flavors_labels[i] for i in cos(image_features[i], flavors_features_array).topk(3).indices])
            prompt = f", {medium}, {movement}, {flaves}" + ', fine details, masterpiece'
            clip_texts.append(prompt)

    print(clip_texts[:5])

    print(f'Clip time: {time.time() - start_time:.2f} s')

    if device == 'cuda':  # should free up ~5k memory
        clip_model.to('cpu')
        del clip_model
        torch.cuda.empty_cache()
        import gc
        gc.collect()


    # Sentence Transformer:
    st_model = SentenceTransformer(CFG.sentence_model_path)

    # COCA + BLIP

    # COCAS:

    # COCA 1:
    coca_model_1 = open_clip.create_model(CFG.coca_model_1_name)
    open_clip.load_checkpoint(coca_model_1, CFG.coca_model_1_checkpoint_path)
    coca_model_1.to(device).eval()  # 2560mb
    print('coca 1 params:', sum(p.numel() for p in coca_model_1.parameters() if p.requires_grad))
    coca_prompts_1 = []

    coca_transform = open_clip.image_transform(
        coca_model_1.visual.image_size,
        is_train = False,
        mean = coca_model_1.visual.image_mean,
        std = coca_model_1.visual.image_std,
    )


    #timing:
    start_time = time.time()
    print('coca 1 bs', CFG.coca_1_bs)

    for batch in tqdm(make_batches(images, CFG.coca_1_bs)):
        coca_in_values = []
        for i, image_name in enumerate(batch):
            img = Image.open(os.path.join(images_dir, image_name)).convert("RGB")
            coca_transformed_img = coca_transform(img).unsqueeze(0)
            coca_in_values.append(coca_transformed_img)

        coca_in_values = torch.cat(coca_in_values, dim=0)

        with torch.no_grad(), torch.cuda.amp.autocast():
            coca_in_values = coca_in_values.to(device)

            # code inside changed: num_beam_hyps_to_keep=3 in beam_scorer, returns sequences and scores
            coca_generated, scores_large = coca_model_1.generate(
                coca_in_values,
                min_seq_len=CFG.min_length,
                seq_len=CFG.max_length,
                num_beam_hyps_to_keep=CFG.num_return_sequences,
                num_beams=CFG.num_beams,
                num_beam_groups=CFG.num_beam_groups,
            )

            coca_decoded = [open_clip.decode(x) for x in coca_generated]

            for coca_prompt in coca_decoded:
                coca_prompt = (
                    coca_prompt
                    .split("<end_of_text>")[0]
                    .replace("<start_of_text>", "")
                    .rstrip(" .,")
                )
                coca_prompts_1.append(coca_prompt)

    print(f'Coca 1 time: {time.time() - start_time}')

    if device == 'cuda':  # should free up ~5k memory
        coca_model_1.to('cpu')
        del coca_model_1
        torch.cuda.empty_cache()
        import gc
        gc.collect()

    # COCA 2:
    coca_model_2 = open_clip.create_model(CFG.coca_model_2_name)
    open_clip.load_checkpoint(coca_model_2, CFG.coca_model_2_checkpoint_path)
    coca_model_2.to(device).eval()  # 1670mb
    print('coca 2 params:', sum(p.numel() for p in coca_model_2.parameters() if p.requires_grad))
    coca_prompts_2 = []

    #timing:
    start_time = time.time()

    print('coca 2 bs', CFG.coca_2_bs)
    for batch in tqdm(make_batches(images, CFG.coca_2_bs)):
        coca_in_values = []
        for i, image_name in enumerate(batch):
            img = Image.open(os.path.join(images_dir, image_name)).convert("RGB")
            coca_transformed_img = coca_transform(img).unsqueeze(0)
            coca_in_values.append(coca_transformed_img)

        coca_in_values = torch.cat(coca_in_values, dim=0)

        with torch.no_grad(), torch.cuda.amp.autocast():
            coca_in_values = coca_in_values.to(device)

            # code inside changed: num_beam_hyps_to_keep=3 in beam_scorer, returns sequences and scores
            coca_generated, scores_base = coca_model_2.generate(
                coca_in_values,
                min_seq_len=CFG.min_length,
                seq_len=CFG.max_length,
                num_beam_hyps_to_keep=CFG.num_return_sequences,
                num_beams=CFG.num_beams,
                num_beam_groups=CFG.num_beam_groups,
            )

            coca_decoded = [open_clip.decode(x) for x in coca_generated]

            for coca_prompt in coca_decoded:
                coca_prompt = (
                    coca_prompt
                    .split("<end_of_text>")[0]
                    .replace("<start_of_text>", "")
                    .rstrip(" .,")
                )
                coca_prompts_2.append(coca_prompt)

    print(f'Coca 2 time: {time.time() - start_time}')

    if device == 'cuda':  # should free up ~5k memory
        coca_model_2.to('cpu')
        del coca_model_2
        torch.cuda.empty_cache()
        import gc
        gc.collect()


    # Blips:

    # LARGE
    blip_processor_large = AutoProcessor.from_pretrained(CFG.blip_model_large_path)
    blip_model_large = BlipForConditionalGeneration.from_pretrained(CFG.blip_model_large_path)
    blip_model_large.to(device).eval() # 1814mb
    blip_out_large = []
    print('blip large params:', sum(p.numel() for p in blip_model_large.parameters() if p.requires_grad))
    start_time = time.time()

    for batch in tqdm(make_batches(images, CFG.blip_large_bs)):
        raw_images = []
        for i, image_name in enumerate(batch):
            img = Image.open(os.path.join(images_dir, image_name)).convert("RGB")
            raw_images.append(img)

        blip_large_in_values = blip_processor_large(images=raw_images, return_tensors="pt").pixel_values

        with torch.no_grad(), torch.cuda.amp.autocast():
            blip_large_in_values = blip_large_in_values.to(device)
            for pv_large_one in blip_large_in_values:
                pv_large_one = pv_large_one.unsqueeze(0)
                # pv_large_one.shape == 1 x nch x dim x dim

                blip_out_large_one = blip_model_large.generate(
                    pixel_values=pv_large_one,
                    max_length=CFG.max_length,
                    min_length=CFG.min_length,
                    num_beams=CFG.num_beams,
                    num_return_sequences=CFG.num_return_sequences,
                    no_repeat_ngram_size=1,
                    remove_invalid_values=True,
                )
                # out_large_one.shape == num_return_seqs x n_tokens

                blip_out_large.extend(blip_out_large_one)

    print(f'blip large time: {time.time() - start_time}')

    if device == 'cuda':  # should free up ~5k memory
        blip_model_large.to('cpu')
        del blip_model_large
        torch.cuda.empty_cache()
        import gc
        gc.collect()


    # BLIP BASE
    blip_processor_base = AutoProcessor.from_pretrained(CFG.blip_model_base_path)
    blip_model_base = BlipForConditionalGeneration.from_pretrained(CFG.blip_model_base_path)
    blip_model_base.to(device).eval() # 1020mb
    blip_out_base = []
    # number of blip params:
    print('blip base params:', sum(p.numel() for p in blip_model_base.parameters() if p.requires_grad))

    #timing:
    start_time = time.time()

    for batch in tqdm(make_batches(images, CFG.blip_base_bs)):
        raw_images = []
        for i, image_name in enumerate(batch):
            img = Image.open(os.path.join(images_dir, image_name)).convert("RGB")
            raw_images.append(img)

        blip_base_in_values = blip_processor_base(images=raw_images, return_tensors="pt").pixel_values

        with torch.no_grad(), torch.cuda.amp.autocast():
            blip_base_in_values = blip_base_in_values.to(device)

            for pv_base_one in blip_base_in_values:
                pv_base_one = pv_base_one.unsqueeze(0)
                blip_out_base_one = blip_model_base.generate(
                    pixel_values=pv_base_one,
                    max_length=CFG.max_length,
                    min_length=CFG.min_length,
                    num_beams=CFG.num_beams,
                    num_return_sequences=CFG.num_return_sequences,
                    no_repeat_ngram_size=1,
                    remove_invalid_values=True,
                )
                blip_out_base.extend(blip_out_base_one)

    print(f'blip base time: {time.time() - start_time}')

    start_time = time.time()

    # POSTPROCESS BLIP
    # decode token ids to actual text prompts
    blip_prompts_large = blip_processor_large.batch_decode(blip_out_large, skip_special_tokens=True)
    blip_prompts_base = blip_processor_base.batch_decode(blip_out_base, skip_special_tokens=True)

    # FINAL POSTPROCESS
    # Add prompt-enhancing words
    blip_prompts_large = [x + y for x, y in zip(blip_prompts_large, clip_texts * CFG.num_return_sequences)]
    blip_prompts_base = [x + y for x, y in zip(blip_prompts_base, clip_texts * CFG.num_return_sequences)]
    coca_prompts_2 = [x + y for x, y in zip(coca_prompts_2, clip_texts * CFG.num_return_sequences)]
    coca_prompts_1 = [x + y for x, y in zip(coca_prompts_1, clip_texts * CFG.num_return_sequences)]

    # Remove trash words from the beginning
    blip_prompts_large = [x.replace('there is', '').replace('this is', '').strip() for x in blip_prompts_large]
    blip_prompts_base = [x.replace('there is', '').replace('this is', '').strip() for x in blip_prompts_base]
    coca_prompts_2 = [x.replace('there is', '').replace('this is', '').strip() for x in coca_prompts_2]
    coca_prompts_1 = [x.replace('there is', '').replace('this is', '').strip() for x in coca_prompts_1]

    # Calculate embeddings for submission
    coca_prompt_embeddings_base = st_model.encode(coca_prompts_2)
    coca_prompt_embeddings_large = st_model.encode(coca_prompts_1)

    blip_prompt_embeddings_base = st_model.encode(blip_prompts_base)
    blip_prompt_embeddings_large = st_model.encode(blip_prompts_large)

    print(f'Decoding time: {time.time() - start_time}')

    start_time = time.time()

    submissions = []
    for i in range(0, len(coca_prompt_embeddings_base), CFG.num_return_sequences):
        one_img_coca_base_embs = coca_prompt_embeddings_base[i:i+CFG.num_return_sequences]
        one_img_coca_large_embs = coca_prompt_embeddings_large[i:i+CFG.num_return_sequences]

        one_img_blip_base_embs = blip_prompt_embeddings_base[i:i+CFG.num_return_sequences]
        one_img_blip_large_embs = blip_prompt_embeddings_large[i:i+CFG.num_return_sequences]

        one_img_embs = np.vstack([
            one_img_coca_large_embs,
            one_img_coca_base_embs,
            one_img_blip_base_embs,
            one_img_blip_large_embs
        ])

        one_img_emb = np.mean(one_img_embs, axis=0)
        submissions.extend(one_img_emb)

    submission = pd.DataFrame(
        index=imgId_eId,
        data=submissions,
        columns=['val']
    ).rename_axis('imgId_eId')

    submission.to_csv(os.path.join(output_dir, 'submission.csv'))

    print(f'Ansambling time: {time.time() - start_time}')

# transforms are basically the same, but not exaclty, so let's use separate processors just in case


if __name__ == '__main__':
    main()


# # from sentence_transformers import SentenceTransformer, models

# comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')